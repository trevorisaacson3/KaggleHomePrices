---
title: "Kaggle: House Prices with Advanced Regression Techniques"
author: "Trevor Isaacson, Hawas Alsadeed, Evan Kessler"
date: "2/28/2022"
output: pdf_document
---

```{r setup, include=FALSE}
set.seed(478)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
library(ggplot2)
library(tidyverse)
library(tidyr)
library(rmarkdown)
library(e1071)
library(glmnet)
library(knitr)
library(leaps)
library(tree)
library(dplyr)
library(caret)
library(gbm)
library(randomForest)
library(GGally)
library(pls)
library(splines)
library(boot)
```

```{r}
trainData = read.csv("train.csv")
testData = read.csv("test.csv")
```

## Introduction
|   Imagine you're looking to purchase a home in the near future.  How do you know the price of that home is accurate and fair.  What factors or variables are you looking at to gauge the final price?  For many, home prices are a number associated with a few key variables.  For example, many buyers are looking at the number of bedrooms, bathrooms and the color of the fence.  What other things affect the price of a home?  Location, building materials, condition and quality are just a few that could potentially impact the price of a house.  In this Kaggle competition, we look at home prices in Ames, Iowa and over 80 variables associated with those homes.  





## Problem Discription
|   Can we accurately predict the price of a home?  




## Data Cleaning

|   Because the original sales price data is very heavily skewed, we needed to log transform the prices.  As shown in the histogram, we have a very heavy right tail because there are a  some listings with very high prices compared to the median price of $180,921 (blue line).  This non-normal shape and distribution is clearly evident in the Q-Q plot.  By applying a log transformation, we fix this problem. 
|   Additionally, after looking at several different variables we noticed there were some containing upwards of 1400 NA or 0 values in a dataset of `r nrow(testData)` observations. For some of them it made sense; such as fireplaces or half baths, a 0 value in this place just means the house simply doesn't have it in which many houses may not. In others however we noticed an issue could arise in fitting our models. We got rid of some of the values that were character values that had over 1000 of 1460 observations being NA. We figured this many values with NAs proved no relevance to our models, and only clogged up our gamms and trees with unncessary fittings. The other changes we made to our datasets was getting rid of NAs and categorizing a value of 'other' for variables with less than 5 values including Neighborhood, Exterior1st and Exterior2nd. Otherwise we split our trianing data into a training and testing set 70% to 30% in order to check our models before submitting the final testing data to kaggle.


```{r echo = FALSE, message=FALSE, fig.width=5, fig.height=3,  fig.show='hold', out.width="50%"}
ggplot(trainData, aes(x=SalePrice)) + 
 geom_histogram(aes(y=..density..), bins = 35, colour="black", fill="white") +
 geom_density(alpha=.2, fill="#FF6666") + 
 geom_vline(xintercept = median(trainData$SalePrice), color = "blue", lty = 2) +
 labs(x = "Sale Price", y = "", title = "Distribution of Sale Price")

ggplot(data = trainData, aes(sample = SalePrice)) +
  stat_qq() + stat_qq_line() + ggtitle("Q-Q Plot of Original Sale Price") 
```


```{r echo = FALSE, message=FALSE, fig.width=5, fig.height=3,  fig.show='hold', out.width="50%"}
ggplot(trainData, aes(x=log(SalePrice))) + 
 geom_histogram(aes(y=..density..), bins = 35, colour="black", fill="white") +
 geom_density(alpha=.2, fill="#FF6666") + 
 geom_vline(xintercept = median(log(trainData$SalePrice)), color = "blue", lty = 2) +
 labs(x = "Sale Price", y = "", title = "Log Distribution of Sale Price")

ggplot(data = trainData, aes(sample = log(SalePrice))) +
  stat_qq() + stat_qq_line() + ggtitle("Q-Q Plot of Log Sale Price") 
```

```{r}
## Changing Dataset
# Easier to chnage values if character instead of factor 
trainData = trainData %>%
  select(-c(Id, Utilities, Condition2, RoofMatl)) %>%
  mutate(SalePrice = log(SalePrice)) %>%
  mutate(FullBath = factor(FullBath)) %>%
  mutate(HalfBath = factor(HalfBath)) %>%
  mutate(Fireplaces = factor(Fireplaces)) %>%
  mutate(BsmtHalfBath = factor(BsmtHalfBath)) %>%
  mutate(FireplaceQu = as.character(FireplaceQu), Electrical = as.character(Electrical), BsmtQual = as.character(BsmtQual), BsmtCond = as.character(BsmtCond)) %>%
  mutate(BsmtExposure = as.character(BsmtExposure), BsmtFinType1 = as.character(BsmtFinType1), BsmtFinType2 = as.character(BsmtFinType2)) %>%
  mutate(GarageType = as.character(GarageType), GarageFinish = as.character(GarageFinish), GarageCond = as.character(GarageCond), GarageQual = as.character(GarageQual)) %>%
  mutate(MasVnrType = as.character(MasVnrType))
  

cond <- (colSums(is.na(trainData)) < 1000) # eliminates variables with over 1000 NA values
trainData <- trainData[, cond, drop = TRUE]
  
trainData = trainData %>%
  mutate(across(where(is.character), replace_na, "None")) %>%
  mutate(across(where(is.integer), replace_na, 0))

# colSums(is.na(trainData))
  
# cond2 <- colSums(trainData == is.integer(0)) # check int variables for values equal to 0
# colSums(is.na(trainData))
```


```{r}
# some variables have values with less than 5 values, so I bundled them into "Other"
trainData = trainData %>%
  mutate(Condition1 = fct_lump(Condition1, n = 5, other_level = "Other")) %>%
  mutate(Neighborhood = fct_lump(Neighborhood, n = 5, other_level = "Other")) %>%
  mutate(Exterior1st = fct_lump(Exterior1st, n = 5, other_level = "Other")) %>%
  mutate(Exterior2nd = fct_lump(Exterior2nd, n = 5, other_level = "Other"))
```


```{r}
# split into training and testing set
trn = sample(seq_len(nrow(trainData)), 0.7* round(nrow(trainData)))
training = trainData[trn, ]
testing = trainData[-trn, ]
```

## Multiple Linear Regression           
|   To begin, we started with a simple multiple linear regression model.  We wanted to give ourselves a baseline root mean squared error value and because linear regression is the easiest to apply and interpret, we determined this is the best place to start.  The model is fit using 73 variables and the training set.  
```{r}
linear = lm(SalePrice ~ ., data = training)
summary(linear)
linear_predict = predict(linear, newdata = testing, type = "response")
MSE_testing = (testing$SalePrice - linear_predict)^2
print(paste("MSE of Testing Set: ", round(sqrt(mean(MSE_testing)), 3)))
```



## Splines        
|   After finding a baseline using linear regression, the next idea was to try fitting regression splines. When looking at some of the more explanatory variables, it was decided to perform a spline on the variables OverallCond, GarageCars, X1stFlrSF and X2ndFlrSF.  
```{r}
#spline on X2ndFlrSF
fit0 <- glm(SalePrice ~ bs(X2ndFlrSF, df = 2), data = training)
cvs0 <- cv.glm(testing, fit0, K = 10)$delta[1]

fit <- glm(SalePrice ~ bs(X2ndFlrSF, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(SalePrice ~ bs(X2ndFlrSF, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(SalePrice ~ bs(X2ndFlrSF, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]
  
degfree <- c(2, 3, 4, 5)
cv <- c(cvs0, cvs, cvs2, cvs3)
df_X2ndFlrSF <- data.frame(degfree, sqrt(cv))
# df_X2ndFlrSF

#spline on OverallCond
fit0 <- glm(SalePrice ~ bs(OverallCond, df = 2), data = training)
cvs0 <- cv.glm(testing, fit0, K = 10)$delta[1]

fit <- glm(SalePrice ~ bs(OverallCond, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(SalePrice ~ bs(OverallCond, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(SalePrice ~ bs(OverallCond, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]
  
degfree <- c(2, 3, 4, 5)
cv <- c(cvs0, cvs, cvs2, cvs3)
df_overallCond <- data.frame(degfree, sqrt(cv))
# df_overallCond

#spline on X1stFlrSF
fit0 <- glm(SalePrice ~ bs(X1stFlrSF, df = 2), data = training)
cvs0 <- cv.glm(testing, fit0, K = 10)$delta[1]

fit <- glm(SalePrice ~ bs(X1stFlrSF, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(SalePrice ~ bs(X1stFlrSF, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(SalePrice ~ bs(X1stFlrSF, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]
  
degfree <- c(2, 3, 4, 5)
cv <- c(cvs0, cvs, cvs2, cvs3)
df_X1stFlrSF <- data.frame(degfree, sqrt(cv))
# df_X1stFlrSF

#spline on GarageCars
fit0 <- glm(SalePrice ~ bs(GarageCars, df = 2), data = training)
cvs0 <- cv.glm(testing, fit0, K = 10)$delta[1]

fit <- glm(SalePrice ~ bs(GarageCars, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(SalePrice ~ bs(GarageCars, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(SalePrice ~ bs(GarageCars, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]
  
degfree <- c(2, 3, 4, 5)
cv <- c(cvs0, cvs, cvs2, cvs3)
df_GarageCars <- data.frame(degfree, sqrt(cv))
# df_GarageCars


splines = c("X2ndFlrSF", "OverallCond", "X1stFlrSF", "GarageCars")
RMSEnumbers = c(df_X2ndFlrSF[1,2], df_overallCond[2,2], df_X1stFlrSF[4, 2], df_GarageCars[2,2])

finalRMSEsplines = data.frame(Splines = splines, RMSE = round(RMSEnumbers,4), RMSE_Dollars = round(exp(RMSEnumbers), 2))
finalRMSEsplines %>%
  kable()
```

# GAM Model

```{r echo = FALSE}
gammod <- lm(SalePrice ~ . + bs(X2ndFlrSF, df = 2) + bs(OverallCond, df = 3) + bs(X1stFlrSF, df = 5) + bs(GarageCars, df = 3) , data = training)
#summary(gammod)
gam_predict = predict(gammod, newdata = testing)
gam_MSE = round(sqrt(mean((testing$SalePrice - gam_predict)^2)), 4)
print(paste("Test RMSE of GAM: ", gam_MSE))
```










## Trees, Bagging, and Random Forests

|   Finally we are doing to try and do trees, bagging, and random forests. To begin we fit a normal tree without any editing. From this it returned 6 significant variables out of the 80 we are testing. These included: OverallQual, GrLivArea, TotalBsmtSF, OverallCond, BsmtFinSF1, and GarageCars yielding 12 terminal nodes and a root mean squared error of .2135 (not the best). After this we looked to prune the tree to see what the best utilization of terminal nodes. After pruning we graphed the RMSE to terminal nodes to discover that anything above 6 would yield similar results. We noticed that throughout all of our terminal node sizes nothing we changed it to would change our RMSE significantly enough to care. Next we tried our luck at bagging.

```{r, echo = FALSE}
tree <- tree(SalePrice ~ ., data = training); summary(tree)
plot(tree); text(tree)

pred <- predict(tree, testing)
sqrt(mean((testing$SalePrice - pred)^2))


tree.cv <- cv.tree(tree)
plot(tree.cv$size, sqrt(tree.cv$dev / nrow(training)), type = "b",
     xlab = "Tree Size", ylab = "CV-RMSE")
```

|   Before we start bagging we need to make sure to change all of our character values to factors. This is because our gbm model stand for gaussian bagging model and it doesn't take characters. In bagging we do it a little bit different than trees. We are not going to start out with a very basic model and prune it, we are instead going to use some of the information we learned from the previous trees. For example, our value for cv.folds in our gaussian bagging model holds the value of 7. This was chosen based off our CV.RMSE values in our graph above as 7 was around when the value of tree sizes begun to fall off and not change too significantly. The value shrinkage we left at the basic value of .1 as we didn't want to unneccesarily use too many trees. We set our trees at a large value of `r nrow(trainData)` in order for the bagging model to use as many trees as necessary (most of the time it sets aound 400-1000 before not reaching the shrinkage paramater). Finally, the last variable we used in our model was 'n.minobsinnode'; this was used as a constraint on the minimum number of observations in the terminal nodes of the tree. In order to find the best model we looped through n.minobsinnode being 1 through 15 and reported the best RMSE value. Below we will return three different graph. The first being a graph depicting the RMSE values of each bagging model. From this graph we can see that a value of 5 for our n.minobsinnode would return the lowest RMSE. Knowing this, we redid the model with this value in place and graphed the actual values of SalePrice (depicted in blue) and where are model went through predicting. 

```{r, echo = FALSE}
set.seed(478)
training <- training %>% mutate_if(is_character, as.factor)
testing <- testing %>% mutate_if(is_character, as.factor)
mino <- c(1:15)
rmse <- rep(NA, length(mino))

for(i in mino) {
  model_gbm <- gbm(SalePrice ~ ., data = training, cv.folds = 7, shrinkage = .1, 
                   n.minobsinnode = i, n.trees = nrow(train), verbose = FALSE)
  
  pred <- predict.gbm(model_gbm, testing)
  rmse[i] <- RMSE(testing$SalePrice, pred)
}

plot(mino, rmse, xlab = "Min Observations per Node", ylab = "RMSE", col = ifelse(rmse == min(rmse), "red", "black"))
lines(mino, rmse)

model_gbm <- gbm(SalePrice ~ ., data = training, cv.folds = 7, shrinkage = .1, 
                   n.minobsinnode = 5, n.trees = nrow(training))

pred <- predict.gbm(model_gbm, testing)
x_ax = 1:length(pred)
plot(x_ax, testing$SalePrice, col="blue", pch=20, cex=.9)
lines(x_ax, pred, col="red", pch=20, cex=.9) 
```

|   Just like with our trees we started with the most basic model we could in random forests. Although we did add a few more paramaters. One including the mtry, defined as the number of variables sampled as candidates at each split. We chose this to be the number of our columns divided by 3 because as stated in The Elements of Statistical Learning: Data Mining, Inference, and Prediction written by Trevor Hastie, Robert Tibshirani, Jerome Friedman, the text stated, "For regression, the default value for m is ⌊p/3⌋ and the minimum node size is five" (Haste 610). In doing our forest model we got a RMSE value of .13, considering this isn't as small as our bagging was and the only reason to do forests over bagging is because of overfitting we can conclude that our bagging from before holds our best model. 

```{r, echo = FALSE}
training <- training %>% mutate_if(is.factor, as.character)
testing <- testing %>% mutate_if(is.factor, as.character)

output <- randomForest(SalePrice ~ ., data = training, mtry = ncol(training)/3, importance = TRUE)

pred <- predict(output, newdata = testing)

rmse <- sqrt(mean((testing$SalePrice - pred)^2)); rmse
```

## Sources

Hastie, Trevor, et al. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer, 2017. 





