---
title: "Kaggle: House Prices with Advanced Regression Techniques"
author: "Trevor Isaacson, Hawas Alsadeed, Evan Kessler"
date: "2/28/2022"
output: pdf_document
---

```{r setup, include=FALSE}
set.seed(478)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
library(ggplot2)
library(tidyverse)
library(tidyr)
library(rmarkdown)
library(e1071)
library(glmnet)
library(knitr)
library(leaps)
library(tree)
library(dplyr)
library(caret)
library(gbm)
library(randomForest)
library(GGally)
library(pls)
library(splines)
library(reticulate)
```

```{r}
trainData = read.csv("train.csv")
testData = read.csv("test.csv")
```

## Introduction
|   Imagine you're looking to purchase a home in the near future.  How do you know the price of that home is accurate and fair.  What factors or variables are you looking at to gauge the final price?  For many, home prices are a number associated with a few key variables.  For example, many buyers are looking at the number of bedrooms, bathrooms and the color of the fence.  What other things affect the price of a home?  Location, building materials, condition and quality are just a few that could potentially impact the price of a house.  In this Kaggle competition, we look at home prices in Ames, Iowa and over 80 variables associated with those homes.  





## Problem Discription
|   Can we accurately predict the price of a home?  




## Data Cleaning

|   Because the original sales price data is very heavily skewed, we needed to log transform the prices.  As shown in the histogram, we have a very heavy right tail because there are a  some listings with very high prices compared to the median price of $180,921 (blue line).  This non-normal shape and distribution is clearly evident in the Q-Q plot.  By applying a log transformation, we fix this problem. 



```{r echo = FALSE, message=FALSE, fig.width=5, fig.height=3,  fig.show='hold', out.width="50%"}
ggplot(trainData, aes(x=SalePrice)) + 
 geom_histogram(aes(y=..density..), bins = 35, colour="black", fill="white") +
 geom_density(alpha=.2, fill="#FF6666") + 
 geom_vline(xintercept = median(trainData$SalePrice), color = "blue", lty = 2) +
 labs(x = "Sale Price", y = "", title = "Distribution of Sale Price")

ggplot(data = trainData, aes(sample = SalePrice)) +
  stat_qq() + stat_qq_line() + ggtitle("Q-Q Plot of Original Sale Price") 
```


```{r echo = FALSE, message=FALSE, fig.width=5, fig.height=3,  fig.show='hold', out.width="50%"}
ggplot(trainData, aes(x=log(SalePrice))) + 
 geom_histogram(aes(y=..density..), bins = 35, colour="black", fill="white") +
 geom_density(alpha=.2, fill="#FF6666") + 
 geom_vline(xintercept = median(log(trainData$SalePrice)), color = "blue", lty = 2) +
 labs(x = "Sale Price", y = "", title = "Log Distribution of Sale Price")

ggplot(data = trainData, aes(sample = log(SalePrice))) +
  stat_qq() + stat_qq_line() + ggtitle("Q-Q Plot of Log Sale Price") 
```

```{r}
## Changing Dataset
# Easier to chnage values if character instead of factor 
trainData = trainData %>%
  select(-c(Id, Utilities, Condition2, RoofMatl)) %>%
  mutate(SalePrice = log(SalePrice)) %>%
  mutate(FullBath = factor(FullBath)) %>%
  mutate(HalfBath = factor(HalfBath)) %>%
  mutate(Fireplaces = factor(Fireplaces)) %>%
  mutate(BsmtHalfBath = factor(BsmtHalfBath)) %>%
  mutate(FireplaceQu = as.character(FireplaceQu), Electrical = as.character(Electrical), BsmtQual = as.character(BsmtQual), BsmtCond = as.character(BsmtCond)) %>%
  mutate(BsmtExposure = as.character(BsmtExposure), BsmtFinType1 = as.character(BsmtFinType1), BsmtFinType2 = as.character(BsmtFinType2)) %>%
  mutate(GarageType = as.character(GarageType), GarageFinish = as.character(GarageFinish), GarageCond = as.character(GarageCond), GarageQual = as.character(GarageQual)) %>%
  mutate(MasVnrType = as.character(MasVnrType))
  

cond <- (colSums(is.na(trainData)) < 1000) # eliminates variables with over 1000 NA values
trainData <- trainData[, cond, drop = TRUE]
  
trainData = trainData %>%
  mutate(across(where(is.character), replace_na, "None")) %>%
  mutate(across(where(is.integer), replace_na, 0))

# colSums(is.na(trainData))
  
# cond2 <- colSums(trainData == is.integer(0)) # check int variables for values equal to 0
# colSums(is.na(trainData))
```


```{r}
# some variables have values with less than 5 values, so I bundled them into "Other"
trainData = trainData %>%
  mutate(Condition1 = fct_lump(Condition1, n = 5, other_level = "Other")) %>%
  mutate(Neighborhood = fct_lump(Neighborhood, n = 5, other_level = "Other")) %>%
  mutate(Exterior1st = fct_lump(Exterior1st, n = 5, other_level = "Other")) %>%
  mutate(Exterior2nd = fct_lump(Exterior2nd, n = 5, other_level = "Other"))
```


```{r}
# split into training and testing set
trn = sample(seq_len(nrow(trainData)), 0.7* round(nrow(trainData)))
training = trainData[trn, ]
testing = trainData[-trn, ]
```


## Multiple Linear Regression
```{r}
linear = lm(SalePrice ~ ., data = training)
summary(linear)
linear_predict = predict(linear, newdata = testing, type = "response")
MSE_testing = (testing$SalePrice - linear_predict)^2
print(paste("MSE of Testing Set: ", round(sqrt(mean(MSE_testing)), 3)))
```



## Splines and GAMs
```{r}
#spline on X2ndFlrSF
fit0 <- glm(SalePrice ~ bs(X2ndFlrSF, df = 2), data = training)
cvs0 <- cv.glm(testing, fit0, K = 10)$delta[1]

fit <- glm(SalePrice ~ bs(X2ndFlrSF, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(SalePrice ~ bs(X2ndFlrSF, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(SalePrice ~ bs(X2ndFlrSF, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]
  
degfree <- c(2, 3, 4, 5)
cv <- c(cvs0, cvs, cvs2, cvs3)
df_MSZoning <- data.frame(degfree, cv)
df_MSZoning

#spline on OverallCond
fit0 <- glm(SalePrice ~ bs(OverallCond, df = 2), data = training)
cvs0 <- cv.glm(testing, fit0, K = 10)$delta[1]

fit <- glm(SalePrice ~ bs(OverallCond, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(SalePrice ~ bs(OverallCond, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(SalePrice ~ bs(OverallCond, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]
  
degfree <- c(2, 3, 4, 5)
cv <- c(cvs0, cvs, cvs2, cvs3)
df_overallCond <- data.frame(degfree, cv)
df_overallCond

#spline on X1stFlrSF
fit0 <- glm(SalePrice ~ bs(X1stFlrSF, df = 2), data = training)
cvs0 <- cv.glm(testing, fit0, K = 10)$delta[1]

fit <- glm(SalePrice ~ bs(X1stFlrSF, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(SalePrice ~ bs(X1stFlrSF, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(SalePrice ~ bs(X1stFlrSF, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]
  
degfree <- c(2, 3, 4, 5)
cv <- c(cvs0, cvs, cvs2, cvs3)
df_X1stFlrSF <- data.frame(degfree, cv)
df_X1stFlrSF

#spline on GarageCars
fit0 <- glm(SalePrice ~ bs(GarageCars, df = 2), data = training)
cvs0 <- cv.glm(testing, fit0, K = 10)$delta[1]

fit <- glm(SalePrice ~ bs(GarageCars, df = 3), data = training)
cvs <- cv.glm(testing, fit, K = 10)$delta[1]

fit2 <- glm(SalePrice ~ bs(GarageCars, df = 4), data = training)
cvs2 <- cv.glm(testing, fit2, K = 10)$delta[1]

fit3 <- glm(SalePrice ~ bs(GarageCars, df = 5), data = training)
cvs3 <- cv.glm(testing, fit3, K = 10)$delta[1]
  
degfree <- c(2, 3, 4, 5)
cv <- c(cvs0, cvs, cvs2, cvs3)
df_GarageCars <- data.frame(degfree, cv)
df_GarageCars
```

# GAM Model
```{r echo = FALSE}
gammod <- lm(SalePrice ~ . + bs(X2ndFlrSF, df = 2) + bs(OverallCond, df = 3) + bs(X1stFlrSF, df = 5) + bs(GarageCars, df = 3) , data = training)
summary(gammod)
gam_predict = predict(gammod, newdata = testing)
gam_MSE = round(sqrt(mean((testing$SalePrice - gam_predict)^2)), 4)
print(paste("Test MSE of GAM: ", gam_MSE))
```












## Sources





